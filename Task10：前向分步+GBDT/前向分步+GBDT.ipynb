{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向分步算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回看Adaboost的算法内容，我们需要通过计算M个基本分类器，每个分类器的错误率、样本权重以及模型权重。我们可以认为：Adaboost每次学习单一分类器以及单一分类器的参数(权重)。接下来，我们抽象出Adaboost算法的整体框架逻辑，构建集成学习的一个非常重要的框架----前向分步算法，有了这个框架，我们不仅可以解决分类问题，也可以解决回归问题。                               \n",
    "**(1) 加法模型：**                                 \n",
    "在Adaboost模型中，我们把每个基本分类器合成一个复杂分类器的方法是每个基本分类器的加权和，即：$f(x)=\\sum_{m=1}^{M} \\beta_{m} b\\left(x ; \\gamma_{m}\\right)$，其中，$b\\left(x ; \\gamma_{m}\\right)$为即基本分类器，$\\gamma_{m}$为基本分类器的参数，$\\beta_m$为基本分类器的权重，显然这与第二章所学的加法模型。为什么这么说呢？大家把$b(x ; \\gamma_{m})$看成是即函数即可。                       \n",
    "在给定训练数据以及损失函数$L(y, f(x))$的条件下，学习加法模型$f(x)$就是：                        \n",
    "$$\n",
    "\\min _{\\beta_{m}, \\gamma_{m}} \\sum_{i=1}^{N} L\\left(y_{i}, \\sum_{m=1}^{M} \\beta_{m} b\\left(x_{i} ; \\gamma_{m}\\right)\\right)\n",
    "$$                      \n",
    "通常这是一个复杂的优化问题，很难通过简单的凸优化的相关知识进行解决。前向分步算法可以用来求解这种方式的问题，它的基本思路是：因为学习的是加法模型，如果从前向后，每一步只优化一个基函数及其系数，逐步逼近目标函数，那么就可以降低优化的复杂度。具体而言，每一步只需要优化：                    \n",
    "$$\n",
    "\\min _{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, \\beta b\\left(x_{i} ; \\gamma\\right)\\right)\n",
    "$$                                   \n",
    "**(2) 前向分步算法：**                              \n",
    "给定数据集$T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}$，$x_{i} \\in \\mathcal{X} \\subseteq \\mathbf{R}^{n}$，$y_{i} \\in \\mathcal{Y}=\\{+1,-1\\}$。损失函数$L(y, f(x))$，基函数集合$\\{b(x ; \\gamma)\\}$，我们需要输出加法模型$f(x)$。                         \n",
    "   - 初始化：$f_{0}(x)=0$                           \n",
    "   - 对m = 1,2,...,M:                     \n",
    "      - (a) 极小化损失函数：\n",
    "      $$\n",
    "      \\left(\\beta_{m}, \\gamma_{m}\\right)=\\arg \\min _{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+\\beta b\\left(x_{i} ; \\gamma\\right)\\right)\n",
    "      $$                        \n",
    "      得到参数$\\beta_{m}$与$\\gamma_{m}$                                           \n",
    "      - (b) 更新：                          \n",
    "      $$\n",
    "      f_{m}(x)=f_{m-1}(x)+\\beta_{m} b\\left(x ; \\gamma_{m}\\right)\n",
    "      $$                                       \n",
    "   - 得到加法模型：                           \n",
    "   $$\n",
    "   f(x)=f_{M}(x)=\\sum_{m=1}^{M} \\beta_{m} b\\left(x ; \\gamma_{m}\\right)\n",
    "   $$                                                     \n",
    "\n",
    "这样，前向分步算法将同时求解从m=1到M的所有参数$\\beta_{m}$，$\\gamma_{m}$的优化问题简化为逐次求解各个$\\beta_{m}$，$\\gamma_{m}$的问题。                           \n",
    "**(3) 前向分步算法与Adaboost的关系：**                                 \n",
    "由于这里不是我们的重点，我们主要阐述这里的结论，不做相关证明，具体的证明见李航老师的《统计学习方法》第八章的3.2节。Adaboost算法是前向分步算法的特例，Adaboost算法是由基本分类器组成的加法模型，损失函数为指数损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度提升决策树(GBDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于残差学习的提升树算法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面的学习过程中，我们一直讨论的都是分类树，比如Adaboost算法，并没有涉及回归的例子。在上一小节我们提到了一个加法模型+前向分步算法的框架，那能否使用这个框架解决回归的例子呢？答案是肯定的。接下来我们来探讨下如何使用加法模型+前向分步算法的框架实现回归问题。                                 \n",
    "在使用加法模型+前向分步算法的框架解决问题之前，我们需要首先确定框架内使用的基函数是什么，在这里我们使用决策树分类器。前面第二章我们已经学过了回归树的基本原理，树算法最重要是寻找最佳的划分点，分类树用纯度来判断最佳划分点使用信息增益（ID3算法），信息增益比（C4.5算法），基尼系数（CART分类树）。但是在回归树中的样本标签是连续数值，可划分点包含了所有特征的所有可取的值。所以再使用熵之类的指标不再合适，取而代之的是平方误差，它能很好的评判拟合程度。基函数确定了以后，我们需要确定每次提升的标准是什么。回想Adaboost算法，在Adaboost算法内使用了分类错误率修正样本权重以及计算每个基本分类器的权重，那回归问题没有分类错误率可言，也就没办法在这里的回归问题使用了，因此我们需要另辟蹊径。模仿分类错误率，我们用每个样本的残差表示每次使用基函数预测时没有解决的那部分问题。因此，我们可以得出如下算法：                                  \n",
    "输入数据集$T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}, x_{i} \\in \\mathcal{X} \\subseteq \\mathbf{R}^{n}, y_{i} \\in \\mathcal{Y} \\subseteq \\mathbf{R}$，输出最终的提升树$f_{M}(x)$                             \n",
    "   - 初始化$f_0(x) = 0$                        \n",
    "   - 对m = 1,2,...,M：                  \n",
    "      - 计算每个样本的残差:$r_{m i}=y_{i}-f_{m-1}\\left(x_{i}\\right), \\quad i=1,2, \\cdots, N$                                    \n",
    "      - 拟合残差$r_{mi}$学习一棵回归树，得到$T\\left(x ; \\Theta_{m}\\right)$                        \n",
    "      - 更新$f_{m}(x)=f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right)$\n",
    "   - 得到最终的回归问题的提升树：$f_{M}(x)=\\sum_{m=1}^{M} T\\left(x ; \\Theta_{m}\\right)$                         \n",
    "   \n",
    "下面我们用一个实际的案例来使用这个算法：(案例来源：李航老师《统计学习方法》)                                                             \n",
    "训练数据如下表，学习这个回归问题的提升树模型，考虑只用树桩作为基函数。                                  \n",
    "![jupyter](./1.png)                                             \n",
    "![jupyter](./2.png)                                    \n",
    "![jupyter](./3.png)                              \n",
    "![jupyter](./4.png)                                  \n",
    "![jupyter](./5.png)                             \n",
    "至此，我们已经能够建立起依靠加法模型+前向分步算法的框架解决回归问题的算法，叫提升树算法。那么，这个算法还是否有提升的空间呢？                                       \n",
    "(2) 梯度提升决策树算法(GBDT)：                                \n",
    "提升树利用加法模型和前向分步算法实现学习的过程，当损失函数为平方损失和指数损失时，每一步优化是相当简单的，也就是我们前面探讨的提升树算法和Adaboost算法。但是对于一般的损失函数而言，往往每一步的优化不是那么容易，针对这一问题，我们得分析问题的本质，也就是是什么导致了在一般损失函数条件下的学习困难。对比以下损失函数：                          \n",
    "$$\n",
    "\\begin{array}{l|l|l}\n",
    "\\hline \\text { Setting } & \\text { Loss Function } & -\\partial L\\left(y_{i}, f\\left(x_{i}\\right)\\right) / \\partial f\\left(x_{i}\\right) \\\\\n",
    "\\hline \\text { Regression } & \\frac{1}{2}\\left[y_{i}-f\\left(x_{i}\\right)\\right]^{2} & y_{i}-f\\left(x_{i}\\right) \\\\\n",
    "\\hline \\text { Regression } & \\left|y_{i}-f\\left(x_{i}\\right)\\right| & \\operatorname{sign}\\left[y_{i}-f\\left(x_{i}\\right)\\right] \\\\\n",
    "\\hline \\text { Regression } & \\text { Huber } & y_{i}-f\\left(x_{i}\\right) \\text { for }\\left|y_{i}-f\\left(x_{i}\\right)\\right| \\leq \\delta_{m} \\\\\n",
    "& & \\delta_{m} \\operatorname{sign}\\left[y_{i}-f\\left(x_{i}\\right)\\right] \\text { for }\\left|y_{i}-f\\left(x_{i}\\right)\\right|>\\delta_{m} \\\\\n",
    "& & \\text { where } \\delta_{m}=\\alpha \\text { th-quantile }\\left\\{\\left|y_{i}-f\\left(x_{i}\\right)\\right|\\right\\} \\\\\n",
    "\\hline \\text { Classification } & \\text { Deviance } & k \\text { th component: } I\\left(y_{i}=\\mathcal{G}_{k}\\right)-p_{k}\\left(x_{i}\\right) \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$                           \n",
    "观察Huber损失函数：                            \n",
    "$$\n",
    "L_{\\delta}(y, f(x))=\\left\\{\\begin{array}{ll}\n",
    "\\frac{1}{2}(y-f(x))^{2} & \\text { for }|y-f(x)| \\leq \\delta \\\\\n",
    "\\delta|y-f(x)|-\\frac{1}{2} \\delta^{2} & \\text { otherwise }\n",
    "\\end{array}\\right.\n",
    "$$                                            \n",
    "针对上面的问题，Freidman提出了梯度提升算法(gradient boosting)，这是利用最速下降法的近似方法，利用损失函数的负梯度在当前模型的值$-\\left[\\frac{\\partial L\\left(y, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)}$作为回归问题提升树算法中的残差的近似值，拟合回归树。**与其说负梯度作为残差的近似值，不如说残差是负梯度的一种特例。**                   \n",
    "以下开始具体介绍梯度提升算法：                      \n",
    "输入训练数据集$T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}, x_{i} \\in \\mathcal{X} \\subseteq \\mathbf{R}^{n}, y_{i} \\in \\mathcal{Y} \\subseteq \\mathbf{R}$和损失函数$L(y, f(x))$，输出回归树$\\hat{f}(x)$                              \n",
    "   - 初始化$f_{0}(x)=\\arg \\min _{c} \\sum_{i=1}^{N} L\\left(y_{i}, c\\right)$                     \n",
    "   - 对于m=1,2,...,M：                   \n",
    "      - 对i = 1,2,...,N计算：$r_{m i}=-\\left[\\frac{\\partial L\\left(y_{i}, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)}$                \n",
    "      - 对$r_{mi}$拟合一个回归树，得到第m棵树的叶结点区域$R_{m j}, j=1,2, \\cdots, J$                           \n",
    "      - 对j=1,2,...J，计算：$c_{m j}=\\arg \\min _{c} \\sum_{x_{i} \\in R_{m j}} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+c\\right)$                      \n",
    "      - 更新$f_{m}(x)=f_{m-1}(x)+\\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right)$                    \n",
    "   - 得到回归树：$\\hat{f}(x)=f_{M}(x)=\\sum_{m=1}^{M} \\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right)$\n",
    "\n",
    "下面，我们来使用一个具体的案例来说明GBDT是如何运作的(案例来源：https://blog.csdn.net/zpalyq110/article/details/79527653 )：                             \n",
    "下面的表格是数据：                           \n",
    "![jupyter](./6.png)                                     \n",
    "学习率：learning_rate=0.1，迭代次数：n_trees=5，树的深度：max_depth=3                                       \n",
    "平方损失的负梯度为：\n",
    "$$\n",
    "-\\left[\\frac{\\left.\\partial L\\left(y, f\\left(x_{i}\\right)\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{t-1}(x)}=y-f\\left(x_{i}\\right) \n",
    "$$                        \n",
    "$c=(1.1+1.3+1.7+1.8)/4=1.475，f_{0}(x)=c=1.475$                                                      \n",
    "![jupyter](./8.png)                                  \n",
    "学习决策树，分裂结点：                                          \n",
    "![jupyter](./9.png)                                  \n",
    "![jupyter](./10.png)                            \n",
    "对于左节点，只有0，1两个样本，那么根据下表我们选择年龄7进行划分：                           \n",
    "![jupyter](./11.png)                                    \n",
    "对于右节点，只有2，3两个样本，那么根据下表我们选择年龄30进行划分：                            \n",
    "![jupyter](./12.png)                                \n",
    "![jupyter](./13.png)                             \n",
    "\n",
    "因此根据$\\Upsilon_{j 1}=\\underbrace{\\arg \\min }_{\\Upsilon} \\sum_{x_{i} \\in R_{j 1}} L\\left(y_{i}, f_{0}\\left(x_{i}\\right)+\\Upsilon\\right)$：                                \n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\left(x_{0} \\in R_{11}\\right), \\quad \\Upsilon_{11}=-0.375 \\\\\n",
    "\\left(x_{1} \\in R_{21}\\right), \\quad \\Upsilon_{21}=-0.175 \\\\\n",
    "\\left(x_{2} \\in R_{31}\\right), \\quad \\Upsilon_{31}=0.225 \\\\\n",
    "\\left(x_{3} \\in R_{41}\\right), \\quad \\Upsilon_{41}=0.325\n",
    "\\end{array}\n",
    "$$                                      \n",
    "这里其实和上面初始化学习器是一个道理，平方损失，求导，令导数等于零，化简之后得到每个叶子节点的参数$\\Upsilon$,其实就是标签值的均值。\n",
    "最后得到五轮迭代：                         \n",
    "![jupyter](./14.png)                                 \n",
    "最后的强学习器为：$f(x)=f_{5}(x)=f_{0}(x)+\\sum_{m=1}^{5} \\sum_{j=1}^{4} \\Upsilon_{j m} I\\left(x \\in R_{j m}\\right)$。                           \n",
    "其中：\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "f_{0}(x)=1.475 & f_{2}(x)=0.0205 \\\\\n",
    "f_{3}(x)=0.1823 & f_{4}(x)=0.1640 \\\\\n",
    "f_{5}(x)=0.1476\n",
    "\\end{array}\n",
    "$$                                \n",
    "预测结果为：                       \n",
    "$$\n",
    "f(x)=1.475+0.1 *(0.2250+0.2025+0.1823+0.164+0.1476)=1.56714\n",
    "$$                                      \n",
    "为什么要用学习率呢？这是Shrinkage的思想，如果每次都全部加上（学习率为1）很容易一步学到位导致过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来使用sklearn来使用GBDT：                                \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor                                                 \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gra#sklearn.ensemble.GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GradientBoostingRegressor`参数解释：\n",
    "1. loss：{‘ls’, ‘lad’, ‘huber’, ‘quantile’}, default=’ls’：‘ls’ 指最小二乘回归. ‘lad’ (最小绝对偏差) 是仅基于输入变量的顺序信息的高度鲁棒的损失函数。. ‘huber’ 是两者的结合. ‘quantile’允许分位数回归（用于alpha指定分位数）\n",
    "2. learning_rate：学习率缩小了每棵树的贡献learning_rate。在learning_rate和n_estimators之间需要权衡。\n",
    "3. n_estimators：要执行的提升次数。\n",
    "4. subsample：用于拟合各个基础学习者的样本比例。如果小于1.0，则将导致随机梯度增强。subsample与参数n_estimators。选择会导致方差减少和偏差增加。subsample < 1.0\n",
    "5. criterion：{'friedman_mse'，'mse'，'mae'}，默认='friedman_mse'：“ mse”是均方误差，“ mae”是平均绝对误差。默认值“ friedman_mse”通常是最好的，因为在某些情况下它可以提供更好的近似值。\n",
    "6. min_samples_split：拆分内部节点所需的最少样本数\n",
    "7. min_samples_leaf：在叶节点处需要的最小样本数。\n",
    "8. min_weight_fraction_leaf：在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。\n",
    "9. max_depth：各个回归模型的最大深度。最大深度限制了树中节点的数量。调整此参数以获得最佳性能；最佳值取决于输入变量的相互作用。\n",
    "10. min_impurity_decrease：如果节点分裂会导致杂质的减少大于或等于该值，则该节点将被分裂。\n",
    "11. min_impurity_split：提前停止树木生长的阈值。如果节点的杂质高于阈值，则该节点将分裂\n",
    "12. max_features{‘auto’, ‘sqrt’, ‘log2’}，int或float：寻找最佳分割时要考虑的功能数量：\n",
    "\n",
    "    * 如果为int，则max_features在每个分割处考虑特征。\n",
    "\n",
    "    * 如果为float，max_features则为小数，并 在每次拆分时考虑要素。int(max_features * n_features)\n",
    "\n",
    "    * 如果“auto”，则max_features=n_features。\n",
    "\n",
    "    * 如果是“ sqrt”，则max_features=sqrt(n_features)。\n",
    "\n",
    "    * 如果为“ log2”，则为max_features=log2(n_features)。\n",
    "\n",
    "    * 如果没有，则max_features=n_features。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "    max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43251288877169414"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_regression(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GradientBoostingClassifier`参数解释："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. loss：待优化的损失函数。 “deviance”是指具有概率输出进行分类的偏差（=logistic regression）。“exponential”梯度提升可恢复AdaBoost算法。\n",
    "2. learning_rate：学习率通过`learning_rate`缩小每棵树的贡献。因此需要平衡`learning_rate`和`n_estimators`\n",
    "3. n_estimators：要执行的推进阶段的数量。梯度增强对过拟合具有较强的鲁棒性，因此大量的梯度增强通常能取得较好的效果。\n",
    "4. subsample：用于拟合单个基本学习者的样本比例。如果小于1.0，会导致随机梯度提升。`subsample`与参数`n_estimators`交互。选择`subsample < 1.0`会导致方差的减少和偏差的增加。\n",
    "5. criterion：该函数用来测量分割的质量。支持的标准是`friedman_mse`，即由Friedman提出的具有改进分数的均方误差, `mse`的平均平方误差，和`mae`的平均绝对误差。默认值`friedman_mse`通常是最好的，因为在某些情况下它可以提供更好的近似值。\n",
    "6. min_samples_split：一个叶节点上所需的最小样本数。只有当它在每个左右分支中都留下至少`min_samples_leaf`训练样本时，任何深度的分割点才会被考虑。特别是在回归中有可能会产生平滑模型的效果。\n",
    "    - 如果为int，则认为min_samples_split是最小值。\n",
    "    - 如果为float，min_samples_split则为分数，`ceil(min_samples_split * n_samples)`是每个拆分的最小样本数。\n",
    "7. min_samples_leaf：分割一个内部节点所需的最小样本数:\n",
    "    - 如果为int，则认为`min_samples_leaf`是最小值。\n",
    "    - 如果为float，`min_samples_leaf`则为分数，`ceil(min_samples_leaf * n_samples)`是每个节点的最小样本数。\n",
    "8. min_weight_fraction_leaf：一个叶节点上所需的(所有输入样本的)总权重的最小加权分数。当不提供`sample_weight`时，样本的权重相等。\n",
    "9. max_depth：各个回归模型的最大深度。最大深度限制了树中节点的数量。调整此参数以获得最佳性能；最佳值取决于输入变量的相互作用。\n",
    "10. min_impurity_decrease：如果节点分裂会导致杂质的减少大于或等于该值，则该节点将被分裂。\n",
    "11. min_impurity_split：提前停止树木生长的阈值。如果节点的杂质高于阈值，则该节点将分裂，否则为叶节点。\n",
    "12. init：用于计算初始预测的估计对象。`init`必须提供`fit`和`predict_proba`。如果为`0`，初始的原始预测被设置为零。默认情况下，使用了预测类先验的`DummyEstimator`。\n",
    "13. random_state：控制每个增强迭代器上的树估计器随机种子。此外，它还可以在每次拆分时控制特性的随机排列。如果`n_iter_no_change`不为`None`，它还控制训练数据的随机分割以获得验证集。在多个函数调用之间传递可重复输出的int。\n",
    "14. max_features：寻找最佳分割时要考虑的特征数量：\n",
    "    - 如果为`int`，则`max_features`在每个分割处考虑特征。\n",
    "    - 如果为`float`，`max_features`则为小数，并 在每次拆分时考虑要素。\n",
    "    - 如果为`auto`，则为`max_features=sqrt(n_features)`。\n",
    "    - 如果是`sqrt`，则`max_features=sqrt(n_features)`。\n",
    "    - 如果为`log2`，则为`max_features=log2(n_features)`。\n",
    "    - 如果为`None`，则`max_features=n_features`。\n",
    "\n",
    "    注意：直到找到至少一个有效的节点样本分区，分割的搜索才会停止，即使它需要有效检查多于max_features个数的要素也是如此。\n",
    "15. verbose：启用详细输出。如果是1，那么它会在一段时间内打印进度和性能(树越多，频率越低)。如果大于1，则输出每棵树的进度和性能。\n",
    "16. max_leaf_nodes：以最佳优先的方式使用`max_leaf_nodes`来种植树。最佳节点定义为杂质的相对减少。如果没有，则叶节点数没有限制。\n",
    "17. warm_start:当设置为`True`时，重用前面调用的解决方案来适应并向集成添加更多的评估器，否则，只会拟合完整的新森林。\n",
    "18. validation_fraction：为提前停止而预留作为验证集的训练数据的比例。必须在0和1之间。仅在`n_iter_no_change`被设置为整数时使用。\n",
    "19. n_iter_no_change：`n_iter_no_change`用于决定在验证分数没有提高时是否使用早期停止来终止训练。默认情况下，它被设置为`None`来禁用提前停止。如果设置为一个数字，它将把训练数据的`validation_fraction`大小放在一边作为验证，并在之前所有的`n_iter_no_change`迭代次数中验证分数没有提高时终止训练。分裂是分层的。\n",
    "20. tol:早停机制。当`n_iter_no_change`迭代器(如果设置为一个数字)对`tol`的损失不再增加时，训练停止。\n",
    "21. ccp_alpha:复杂度参数用于最小代价复杂度剪枝。将选择代价复杂度最大且小于`ccp_alpha`的子树。默认情况下，不执行修剪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
